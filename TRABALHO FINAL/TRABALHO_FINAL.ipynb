{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DANIEL SILVEIRA DANTAS\n",
        "# DSD@CESAR.SCHOOL\n",
        "# LINK: https://www.kaggle.com/datasets/mloey1/ahcd1"
      ],
      "metadata": {
        "id": "2k0BLLlUIl4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports:"
      ],
      "metadata": {
        "id": "ljtXlCZQI4iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import copy"
      ],
      "metadata": {
        "id": "FHyzJdQ1AbsO"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregamento do dataset:"
      ],
      "metadata": {
        "id": "Qs5xRLgEJSwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar dataset\n",
        "base_dir = kagglehub.dataset_download(\"mloey1/ahcd1\")\n",
        "print(\"Baixado em:\", base_dir)\n",
        "\n",
        "csv_dir = os.path.join(base_dir, \"Arabic Handwritten Characters Dataset CSV\")\n",
        "print(\"Pasta CSV:\", csv_dir)\n",
        "\n",
        "# Carregar CSVs (imagens e rótulos)\n",
        "train_images = pd.read_csv(os.path.join(csv_dir, \"csvTrainImages 13440x1024.csv\"), header=None)\n",
        "train_labels = pd.read_csv(os.path.join(csv_dir, \"csvTrainLabel 13440x1.csv\"), header=None)\n",
        "\n",
        "test_images  = pd.read_csv(os.path.join(csv_dir, \"csvTestImages 3360x1024.csv\"), header=None)\n",
        "test_labels  = pd.read_csv(os.path.join(csv_dir, \"csvTestLabel 3360x1.csv\"), header=None)\n",
        "\n",
        "print(\"train_images:\", train_images.shape)\n",
        "print(\"train_labels:\", train_labels.shape)\n",
        "print(\"test_images:\",  test_images.shape)\n",
        "print(\"test_labels:\",  test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKIDZQCeJNnY",
        "outputId": "8c3fbc22-8b7b-4c66-cd69-725f04908eab"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ahcd1' dataset.\n",
            "Baixado em: /kaggle/input/ahcd1\n",
            "Pasta CSV: /kaggle/input/ahcd1/Arabic Handwritten Characters Dataset CSV\n",
            "train_images: (13440, 1024)\n",
            "train_labels: (13440, 1)\n",
            "test_images: (3360, 1024)\n",
            "test_labels: (3360, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação dos Dados: Normalização e Ajuste dos Labels"
      ],
      "metadata": {
        "id": "7kUj5ZveKLy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte para float32 e normaliza para [0,1]\n",
        "X_train = torch.tensor(train_images.values, dtype=torch.float32) / 255.0\n",
        "X_test  = torch.tensor(test_images.values,  dtype=torch.float32) / 255.0\n",
        "\n",
        "# Labels são de 1 a 28; CrossEntropyLoss espera 0..27\n",
        "y_train = torch.tensor(train_labels.values.squeeze() - 1, dtype=torch.long)\n",
        "y_test  = torch.tensor(test_labels.values.squeeze()  - 1, dtype=torch.long)\n",
        "\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"y_train:\", y_train.shape, \"labels únicos:\", torch.unique(y_train))\n",
        "print(\"X_test:\",  X_test.shape)\n",
        "print(\"y_test:\",  y_test.shape,  \"labels únicos:\", torch.unique(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MND5bVKDAbiS",
        "outputId": "6e1aac53-2cbd-492f-8489-7dc41ea19a61"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: torch.Size([13440, 1024])\n",
            "y_train: torch.Size([13440]) labels únicos: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\n",
            "X_test: torch.Size([3360, 1024])\n",
            "y_test: torch.Size([3360]) labels únicos: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construção dos Datasets e DataLoaders"
      ],
      "metadata": {
        "id": "h6v6boDBK6ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset  = TensorDataset(X_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Checagem rápida\n",
        "imgs, lbls = next(iter(train_loader))\n",
        "print(\"Batch imagens:\", imgs.shape)  # [batch, 1024]\n",
        "print(\"Batch labels:\", lbls.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9z5QFc2AbdN",
        "outputId": "d939ccc0-361b-4deb-8ed2-5f640d64464b"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch imagens: torch.Size([128, 1024])\n",
            "Batch labels: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definindo o modelo da rede:"
      ],
      "metadata": {
        "id": "MRjHX3jWK-v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "NUM_CLASSES = 28  # 28 letras do alfabeto árabe\n",
        "\n",
        "class MLP_AHCD(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MLP_AHCD, self).__init__()\n",
        "\n",
        "        # Entrada: 1024 pixels (32x32 achatado)\n",
        "        self.fc1 = nn.Linear(32 * 32, 56)   # 1024 -> 28\n",
        "        self.fc2 = nn.Linear(56, 56)        # 28 -> 28   (2ª camada oculta)\n",
        "        self.fc3 = nn.Linear(56, num_classes)  # 28 -> 28 classes\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten (batch_size, 1024)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))    # nova camada oculta\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = MLP_AHCD(NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05e-1, weight_decay=3.625e-4)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqL_aCQyAbTw",
        "outputId": "16e0c2a8-f737-4b0f-b83f-0da7d2e4aa76"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP_AHCD(\n",
              "  (fc1): Linear(in_features=1024, out_features=56, bias=True)\n",
              "  (fc2): Linear(in_features=56, out_features=56, bias=True)\n",
              "  (fc3): Linear(in_features=56, out_features=28, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento:"
      ],
      "metadata": {
        "id": "uJq4I6glLG31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----- EARLY STOPPING CONFIG -----\n",
        "patience = 10\n",
        "best_test_acc = 0.0\n",
        "best_epoch = 0\n",
        "epochs_no_improve = 0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # --------- TREINO ---------\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --------- TESTE ---------\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    test_running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            test_correct += (preds == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "    test_loss = test_running_loss / test_total\n",
        "\n",
        "    print(f\"Época {epoch+1}/{num_epochs} | \"\n",
        "          f\"Treino Acc: {train_acc:.4f} | \"\n",
        "          f\"Teste Acc: {test_acc:.4f} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # --------- EARLY STOPPING CHECK ---------\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch + 1\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # --------- ATUALIZA O LEARNING RATE DO SCHEDULER ---------\n",
        "    scheduler.step()\n",
        "\n",
        "    # --------- VERIFICA SE PARA ---------\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nEarly stopping ativado na época {epoch+1}.\")\n",
        "        print(f\"Melhor Test Acc = {best_test_acc:.4f} (época {best_epoch})\")\n",
        "        break\n",
        "\n",
        "# --------- CARREGA O MELHOR MODELO ---------\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(f\"\\nModelo final carregado da época {best_epoch} \"\n",
        "      f\"com Test Acc = {best_test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3RF0YZNAa4D",
        "outputId": "8946d1ac-c92f-4e74-e440-0efcfd8450d9"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1/100 | Treino Acc: 0.2597 | Teste Acc: 0.4024 | Train Loss: 2.3997 | Test Loss: 1.7989\n",
            "Época 2/100 | Treino Acc: 0.4953 | Teste Acc: 0.5482 | Train Loss: 1.5025 | Test Loss: 1.3119\n",
            "Época 3/100 | Treino Acc: 0.6080 | Teste Acc: 0.6033 | Train Loss: 1.1357 | Test Loss: 1.1342\n",
            "Época 4/100 | Treino Acc: 0.6731 | Teste Acc: 0.6360 | Train Loss: 0.9378 | Test Loss: 1.0329\n",
            "Época 5/100 | Treino Acc: 0.7115 | Teste Acc: 0.6562 | Train Loss: 0.8120 | Test Loss: 0.9961\n",
            "Época 6/100 | Treino Acc: 0.7411 | Teste Acc: 0.6735 | Train Loss: 0.7252 | Test Loss: 0.9538\n",
            "Época 7/100 | Treino Acc: 0.7675 | Teste Acc: 0.6926 | Train Loss: 0.6600 | Test Loss: 0.9203\n",
            "Época 8/100 | Treino Acc: 0.7894 | Teste Acc: 0.7036 | Train Loss: 0.5981 | Test Loss: 0.8838\n",
            "Época 9/100 | Treino Acc: 0.8009 | Teste Acc: 0.6979 | Train Loss: 0.5666 | Test Loss: 0.9192\n",
            "Época 10/100 | Treino Acc: 0.8134 | Teste Acc: 0.6929 | Train Loss: 0.5296 | Test Loss: 0.9082\n",
            "Época 11/100 | Treino Acc: 0.8224 | Teste Acc: 0.7119 | Train Loss: 0.4955 | Test Loss: 0.8965\n",
            "Época 12/100 | Treino Acc: 0.8318 | Teste Acc: 0.6985 | Train Loss: 0.4739 | Test Loss: 0.9470\n",
            "Época 13/100 | Treino Acc: 0.8369 | Teste Acc: 0.7077 | Train Loss: 0.4627 | Test Loss: 0.9162\n",
            "Época 14/100 | Treino Acc: 0.8460 | Teste Acc: 0.7095 | Train Loss: 0.4299 | Test Loss: 0.9122\n",
            "Época 15/100 | Treino Acc: 0.8609 | Teste Acc: 0.7182 | Train Loss: 0.3950 | Test Loss: 0.9146\n",
            "Época 16/100 | Treino Acc: 0.8612 | Teste Acc: 0.7164 | Train Loss: 0.3925 | Test Loss: 0.9118\n",
            "Época 17/100 | Treino Acc: 0.8745 | Teste Acc: 0.7074 | Train Loss: 0.3635 | Test Loss: 0.9255\n",
            "Época 18/100 | Treino Acc: 0.8829 | Teste Acc: 0.7229 | Train Loss: 0.3403 | Test Loss: 0.9255\n",
            "Época 19/100 | Treino Acc: 0.8803 | Teste Acc: 0.7146 | Train Loss: 0.3397 | Test Loss: 0.9634\n",
            "Época 20/100 | Treino Acc: 0.8774 | Teste Acc: 0.6979 | Train Loss: 0.3541 | Test Loss: 1.0135\n",
            "Época 21/100 | Treino Acc: 0.8888 | Teste Acc: 0.7131 | Train Loss: 0.3181 | Test Loss: 0.9725\n",
            "Época 22/100 | Treino Acc: 0.8969 | Teste Acc: 0.7176 | Train Loss: 0.2995 | Test Loss: 0.9844\n",
            "Época 23/100 | Treino Acc: 0.8980 | Teste Acc: 0.6964 | Train Loss: 0.2963 | Test Loss: 1.0785\n",
            "Época 24/100 | Treino Acc: 0.8959 | Teste Acc: 0.7247 | Train Loss: 0.3016 | Test Loss: 0.9915\n",
            "Época 25/100 | Treino Acc: 0.9045 | Teste Acc: 0.7256 | Train Loss: 0.2729 | Test Loss: 0.9812\n",
            "Época 26/100 | Treino Acc: 0.9051 | Teste Acc: 0.7196 | Train Loss: 0.2745 | Test Loss: 0.9989\n",
            "Época 27/100 | Treino Acc: 0.9107 | Teste Acc: 0.7295 | Train Loss: 0.2579 | Test Loss: 1.0019\n",
            "Época 28/100 | Treino Acc: 0.9099 | Teste Acc: 0.7119 | Train Loss: 0.2621 | Test Loss: 1.0819\n",
            "Época 29/100 | Treino Acc: 0.9062 | Teste Acc: 0.7244 | Train Loss: 0.2671 | Test Loss: 1.0218\n",
            "Época 30/100 | Treino Acc: 0.9170 | Teste Acc: 0.7158 | Train Loss: 0.2454 | Test Loss: 1.0465\n",
            "Época 31/100 | Treino Acc: 0.9648 | Teste Acc: 0.7473 | Train Loss: 0.1446 | Test Loss: 0.9304\n",
            "Época 32/100 | Treino Acc: 0.9797 | Teste Acc: 0.7470 | Train Loss: 0.1126 | Test Loss: 0.9179\n",
            "Época 33/100 | Treino Acc: 0.9842 | Teste Acc: 0.7509 | Train Loss: 0.1040 | Test Loss: 0.9194\n",
            "Época 34/100 | Treino Acc: 0.9859 | Teste Acc: 0.7503 | Train Loss: 0.0998 | Test Loss: 0.9153\n",
            "Época 35/100 | Treino Acc: 0.9874 | Teste Acc: 0.7491 | Train Loss: 0.0964 | Test Loss: 0.9183\n",
            "Época 36/100 | Treino Acc: 0.9885 | Teste Acc: 0.7488 | Train Loss: 0.0941 | Test Loss: 0.9176\n",
            "Época 37/100 | Treino Acc: 0.9886 | Teste Acc: 0.7476 | Train Loss: 0.0929 | Test Loss: 0.9214\n",
            "Época 38/100 | Treino Acc: 0.9894 | Teste Acc: 0.7491 | Train Loss: 0.0913 | Test Loss: 0.9202\n",
            "Época 39/100 | Treino Acc: 0.9907 | Teste Acc: 0.7500 | Train Loss: 0.0897 | Test Loss: 0.9204\n",
            "Época 40/100 | Treino Acc: 0.9908 | Teste Acc: 0.7509 | Train Loss: 0.0886 | Test Loss: 0.9225\n",
            "Época 41/100 | Treino Acc: 0.9912 | Teste Acc: 0.7503 | Train Loss: 0.0879 | Test Loss: 0.9239\n",
            "Época 42/100 | Treino Acc: 0.9911 | Teste Acc: 0.7479 | Train Loss: 0.0875 | Test Loss: 0.9246\n",
            "Época 43/100 | Treino Acc: 0.9914 | Teste Acc: 0.7530 | Train Loss: 0.0867 | Test Loss: 0.9234\n",
            "Época 44/100 | Treino Acc: 0.9920 | Teste Acc: 0.7554 | Train Loss: 0.0860 | Test Loss: 0.9294\n",
            "Época 45/100 | Treino Acc: 0.9921 | Teste Acc: 0.7503 | Train Loss: 0.0852 | Test Loss: 0.9331\n",
            "Época 46/100 | Treino Acc: 0.9924 | Teste Acc: 0.7494 | Train Loss: 0.0851 | Test Loss: 0.9338\n",
            "Época 47/100 | Treino Acc: 0.9929 | Teste Acc: 0.7491 | Train Loss: 0.0842 | Test Loss: 0.9365\n",
            "Época 48/100 | Treino Acc: 0.9932 | Teste Acc: 0.7506 | Train Loss: 0.0837 | Test Loss: 0.9367\n",
            "Época 49/100 | Treino Acc: 0.9933 | Teste Acc: 0.7509 | Train Loss: 0.0833 | Test Loss: 0.9389\n",
            "Época 50/100 | Treino Acc: 0.9939 | Teste Acc: 0.7527 | Train Loss: 0.0822 | Test Loss: 0.9416\n",
            "Época 51/100 | Treino Acc: 0.9935 | Teste Acc: 0.7515 | Train Loss: 0.0821 | Test Loss: 0.9461\n",
            "Época 52/100 | Treino Acc: 0.9941 | Teste Acc: 0.7533 | Train Loss: 0.0813 | Test Loss: 0.9416\n",
            "Época 53/100 | Treino Acc: 0.9942 | Teste Acc: 0.7509 | Train Loss: 0.0810 | Test Loss: 0.9391\n",
            "Época 54/100 | Treino Acc: 0.9938 | Teste Acc: 0.7497 | Train Loss: 0.0805 | Test Loss: 0.9472\n",
            "\n",
            "Early stopping ativado na época 54.\n",
            "Melhor Test Acc = 0.7554 (época 44)\n",
            "\n",
            "Modelo final carregado da época 44 com Test Acc = 0.7554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusão:"
      ],
      "metadata": {
        "id": "i0jQwttNeKN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O processo de desenvolvimento e ajuste da rede MLP para reconhecer o alfabeto árabe foi longo e bastante experimental, principalmente por causa de um erro cometido logo no início — algo que só ficou claro no final e que alterou toda a trajetória dos testes. A ordem das modificações seguiu uma sequência lógica, mas muitas vezes precisei voltar atrás e revisar decisões anteriores. Comecei alterando o número de neurônios por camada, depois testei diferentes quantidades de camadas e, em seguida, aumentei o número de épocas para 20. Com esses primeiros experimentos, concluí prematuramente que a melhor configuração era com 2 camadas ocultas e 28 neurônios cada, pois, ao testar 3 camadas ocultas e/ou 56 neurônios com apenas 20 épocas, a rede apresentou underfitting e acabei assumindo, de forma equivocada, que 56 neurônios eram inadequados.\n",
        "### Para tentar extrair mais aprendizado, aumentei o número de épocas para 100. Quando isso não foi suficiente, comecei a ajustar o learning rate, explorando valores maiores e menores até encontrar um valor adequado para a configuração que eu acreditava ser a correta (2 camadas e 28 neurônios). Em seguida, passei ao weight decay, testando diferentes valores para reduzir overfitting. Depois disso, ajustei o dropout, o que trouxe melhora, e só então alterei o batch size, que acabou ficando ótimo em 128. Com esses elementos estabilizados, adicionei um learning rate scheduler (StepLR), ajustei seu gamma e implementei o early stopping.\n",
        "### Mesmo seguindo essa sequência estruturada, o processo esteve longe de ser direto. Houve momentos claros de overfitting, especialmente quando a acurácia de treino superava 0.90 enquanto a acurácia de teste permanecia por volta de 0.60 — o que exigiu ajustes cuidadosos de regularização e dropout. A busca pelos valores ótimos de cada parâmetro sempre envolveu aumentá-los e diminuí-los repetidamente em torno do valor inicial, mantendo todos os outros parâmetros fixos para isolar o efeito de cada mudança. Testei também outras ativações, mas todas pioraram o modelo, reforçando que ReLU era a opção mais adequada.\n",
        "### No fim do processo, depois de ajustar praticamente todos os hiperparâmetros, percebi que havia cometido um erro inicial: eu nunca tinha testado 56 neurônios com 100 épocas, somente com 20. Ao refazer esse teste — o último da sequência — obtive de longe a melhor acurácia de teste, ultrapassando 0.70. Isso mostrou que todos os valores ótimos anteriores não eram os verdadeiros valores ótimos para essa rede, já que eles haviam sido encontrados para uma arquitetura inadequada (28 neurônios), e não para a arquitetura realmente mais promissora (56 neurônios). Concluí, assim, que o número de camadas, de neurônios e de épocas são fatores fundamentais para a qualidade final do modelo, e que eles devem ser sempre revisados com prioridade quando as métricas principais (perda e acurácia de teste para este trabalho) não estiverem convergindo para os valores desejados."
      ],
      "metadata": {
        "id": "AbTROFiweOS7"
      }
    }
  ]
}